{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7222412,"sourceType":"datasetVersion","datasetId":4180521}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ishanmitra96/nlp-fake-news-cnn-lstm-and-transformers?scriptVersionId=175112667\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T14:24:00.119183Z","iopub.execute_input":"2024-05-01T14:24:00.119513Z","iopub.status.idle":"2024-05-01T14:24:01.13806Z","shell.execute_reply.started":"2024-05-01T14:24:00.119488Z","shell.execute_reply":"2024-05-01T14:24:01.137084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset","metadata":{}},{"cell_type":"code","source":"true_df = pd.read_csv(\"/kaggle/input/fake-news-detection/true.csv\")\ntrue_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:24:04.613406Z","iopub.execute_input":"2024-05-01T14:24:04.614223Z","iopub.status.idle":"2024-05-01T14:24:05.992069Z","shell.execute_reply.started":"2024-05-01T14:24:04.614194Z","shell.execute_reply":"2024-05-01T14:24:05.991061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df = pd.read_csv(\"/kaggle/input/fake-news-detection/fake.csv\")\nfake_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:24:07.670405Z","iopub.execute_input":"2024-05-01T14:24:07.671555Z","iopub.status.idle":"2024-05-01T14:24:09.138085Z","shell.execute_reply.started":"2024-05-01T14:24:07.67152Z","shell.execute_reply":"2024-05-01T14:24:09.13701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"https://github.com/remydeshayes/NLP_Pytorch/blob/main/Notebook%20-%20Fake_News%20Detection%20Pytorch%20-%20Billiot_Deshayes.ipynb","metadata":{}},{"cell_type":"markdown","source":"Preprocessing of true and fake datasets for data optimisation and removal of blank and duplicate entries.","metadata":{}},{"cell_type":"markdown","source":"## Verified News Dataset","metadata":{}},{"cell_type":"code","source":"# TRUE DATASET\n# Checking for NaN values\ntrue_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:24:16.406186Z","iopub.execute_input":"2024-05-01T14:24:16.406882Z","iopub.status.idle":"2024-05-01T14:24:16.424957Z","shell.execute_reply.started":"2024-05-01T14:24:16.40685Z","shell.execute_reply":"2024-05-01T14:24:16.424138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df['text']","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:24:17.967744Z","iopub.execute_input":"2024-05-01T14:24:17.968529Z","iopub.status.idle":"2024-05-01T14:24:17.977918Z","shell.execute_reply.started":"2024-05-01T14:24:17.968497Z","shell.execute_reply":"2024-05-01T14:24:17.976882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download stopwords from nltk library\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = list(set(stopwords.words('english')))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:24:51.763682Z","iopub.execute_input":"2024-05-01T14:24:51.76435Z","iopub.status.idle":"2024-05-01T14:24:53.764408Z","shell.execute_reply.started":"2024-05-01T14:24:51.764317Z","shell.execute_reply":"2024-05-01T14:24:53.763547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define function for most common words\n# https://github.com/remydeshayes/NLP_Pytorch/blob/main/Notebook%20-%20Fake_News%20Detection%20Pytorch%20-%20Billiot_Deshayes.ipynb\nfrom collections import Counter\n\n\ndef most_common(corpus, nb_words):\n    articles = corpus.str.split()\n    # Explanation of nested list comprehension:\n    # Iterate through every article in articles\n    # Iterate through every word in article (second for in loop)\n    # Add word to the np.array if it is not a stopword (nltk)\n    words = np.array([word for article in articles for word in article if word.lower() not in stopwords])\n    counter = Counter(words)\n    d = pd.DataFrame(counter, index=['occurences']).transpose().reset_index()\n    d.columns = ['word', 'occurences']\n    d = d.sort_values('occurences', ascending=False)\n    return d[:nb_words]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:24:57.425467Z","iopub.execute_input":"2024-05-01T14:24:57.426374Z","iopub.status.idle":"2024-05-01T14:24:57.432602Z","shell.execute_reply.started":"2024-05-01T14:24:57.426344Z","shell.execute_reply":"2024-05-01T14:24:57.431736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is VERY HIGH cpu compute (but it only takes a few seconds)\nmost_common(true_df['text'], 10)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:24:59.68207Z","iopub.execute_input":"2024-05-01T14:24:59.682424Z","iopub.status.idle":"2024-05-01T14:25:27.902938Z","shell.execute_reply.started":"2024-05-01T14:24:59.682398Z","shell.execute_reply":"2024-05-01T14:25:27.90196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even without computing the occurences, we notice the presence of the format \"CITY (Reuters) - \" at the beginning of each article. This is missing in all the articles from the fake_df.\nWe will delete this pattern format where it is present in the article within the first 50 characters of an article.\nFailing to clean this would create a bias where perhaps, presence of the word Reuters would equate to non-falsified news.","metadata":{}},{"cell_type":"code","source":"import re","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:25:27.904614Z","iopub.execute_input":"2024-05-01T14:25:27.90491Z","iopub.status.idle":"2024-05-01T14:25:27.909144Z","shell.execute_reply.started":"2024-05-01T14:25:27.904885Z","shell.execute_reply":"2024-05-01T14:25:27.908217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0, len(true_df['text'])):\n    try :\n        # Search if CITY (Reuters) exist, if it does, shift the index\n        # to 3 more spaces to accomodate the hyphen\n        start = re.search('(Reuters)',true_df['text'][i][0:49]).end() + 3\n    except:\n        pass\n    else:\n        true_df['text'][i] = true_df['text'][i][start:]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:25:31.70747Z","iopub.execute_input":"2024-05-01T14:25:31.708061Z","iopub.status.idle":"2024-05-01T14:25:38.342876Z","shell.execute_reply.started":"2024-05-01T14:25:31.70803Z","shell.execute_reply":"2024-05-01T14:25:38.341926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:25:42.28196Z","iopub.execute_input":"2024-05-01T14:25:42.28239Z","iopub.status.idle":"2024-05-01T14:25:42.295056Z","shell.execute_reply.started":"2024-05-01T14:25:42.282362Z","shell.execute_reply":"2024-05-01T14:25:42.294168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for duplicates or blank articles\n# Conversion to dataframe to reduce output text clutter\nduplicate = true_df['text'].value_counts()[true_df['text'].value_counts()>1]\nduplicate = duplicate.rename_axis('unique_values').reset_index(name='counts')\nduplicate","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:25:53.396266Z","iopub.execute_input":"2024-05-01T14:25:53.396955Z","iopub.status.idle":"2024-05-01T14:25:53.488291Z","shell.execute_reply.started":"2024-05-01T14:25:53.396924Z","shell.execute_reply":"2024-05-01T14:25:53.487328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of duplicate entries\ntrue_df['text'].value_counts()[true_df['text'].value_counts()>1].sum() - 212","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:25:55.788518Z","iopub.execute_input":"2024-05-01T14:25:55.789335Z","iopub.status.idle":"2024-05-01T14:25:55.823796Z","shell.execute_reply.started":"2024-05-01T14:25:55.789306Z","shell.execute_reply":"2024-05-01T14:25:55.82295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deletion of duplicates\ntrue_df = true_df.drop_duplicates(subset=['text'], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:25:57.43261Z","iopub.execute_input":"2024-05-01T14:25:57.43295Z","iopub.status.idle":"2024-05-01T14:25:57.444687Z","shell.execute_reply.started":"2024-05-01T14:25:57.432924Z","shell.execute_reply":"2024-05-01T14:25:57.44371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:25:58.808649Z","iopub.execute_input":"2024-05-01T14:25:58.809231Z","iopub.status.idle":"2024-05-01T14:25:58.814843Z","shell.execute_reply.started":"2024-05-01T14:25:58.809202Z","shell.execute_reply":"2024-05-01T14:25:58.814059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate title and text column\ntrue_df['article'] = true_df['title'] + '.' + true_df['text']","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:01.403858Z","iopub.execute_input":"2024-05-01T14:26:01.404774Z","iopub.status.idle":"2024-05-01T14:26:01.437214Z","shell.execute_reply.started":"2024-05-01T14:26:01.404741Z","shell.execute_reply":"2024-05-01T14:26:01.436209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df = true_df.drop(['title', 'text'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:04.141953Z","iopub.execute_input":"2024-05-01T14:26:04.142337Z","iopub.status.idle":"2024-05-01T14:26:04.150344Z","shell.execute_reply.started":"2024-05-01T14:26:04.142308Z","shell.execute_reply":"2024-05-01T14:26:04.149423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:05.969665Z","iopub.execute_input":"2024-05-01T14:26:05.970236Z","iopub.status.idle":"2024-05-01T14:26:05.981184Z","shell.execute_reply.started":"2024-05-01T14:26:05.970206Z","shell.execute_reply":"2024-05-01T14:26:05.980292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For future EDA analysis, we keep the date column for plotting metrics. Date is processed into a uniform format.","metadata":{}},{"cell_type":"code","source":"true_df['date_len'] = [len(x) for x in true_df['date']]\nprint(true_df['date_len'].value_counts())\ntrue_df = true_df.drop(['date_len'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:10.159901Z","iopub.execute_input":"2024-05-01T14:26:10.160535Z","iopub.status.idle":"2024-05-01T14:26:10.187519Z","shell.execute_reply.started":"2024-05-01T14:26:10.160504Z","shell.execute_reply":"2024-05-01T14:26:10.186395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:11.89608Z","iopub.execute_input":"2024-05-01T14:26:11.896881Z","iopub.status.idle":"2024-05-01T14:26:11.908053Z","shell.execute_reply.started":"2024-05-01T14:26:11.89685Z","shell.execute_reply":"2024-05-01T14:26:11.907085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:14.89975Z","iopub.execute_input":"2024-05-01T14:26:14.900328Z","iopub.status.idle":"2024-05-01T14:26:14.904724Z","shell.execute_reply.started":"2024-05-01T14:26:14.900287Z","shell.execute_reply":"2024-05-01T14:26:14.903857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All data format is uniform (month date, year)\n# Unifying the date format to datetime\ndates = []\nfor x in true_df['date']:\n    date = datetime.strptime(x,'%B %d, %Y ')\n    dates.append(date)\ntrue_df['date'] = dates","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:22.203714Z","iopub.execute_input":"2024-05-01T14:26:22.204077Z","iopub.status.idle":"2024-05-01T14:26:22.538538Z","shell.execute_reply.started":"2024-05-01T14:26:22.204048Z","shell.execute_reply":"2024-05-01T14:26:22.537792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:24.084426Z","iopub.execute_input":"2024-05-01T14:26:24.085309Z","iopub.status.idle":"2024-05-01T14:26:24.099058Z","shell.execute_reply.started":"2024-05-01T14:26:24.085265Z","shell.execute_reply":"2024-05-01T14:26:24.098022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df['date_len'] = [len(str(x)) for x in true_df['date']]\nprint(true_df['date_len'].value_counts())\ntrue_df = true_df.drop(['date_len'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:27.254537Z","iopub.execute_input":"2024-05-01T14:26:27.254883Z","iopub.status.idle":"2024-05-01T14:26:27.449171Z","shell.execute_reply.started":"2024-05-01T14:26:27.254854Z","shell.execute_reply":"2024-05-01T14:26:27.44821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally creating a label variable - 1 means the news is verified","metadata":{}},{"cell_type":"code","source":"true_df['label'] = 1","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:30.991705Z","iopub.execute_input":"2024-05-01T14:26:30.992063Z","iopub.status.idle":"2024-05-01T14:26:30.997053Z","shell.execute_reply.started":"2024-05-01T14:26:30.992035Z","shell.execute_reply":"2024-05-01T14:26:30.996167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:31.96069Z","iopub.execute_input":"2024-05-01T14:26:31.961072Z","iopub.status.idle":"2024-05-01T14:26:31.973476Z","shell.execute_reply.started":"2024-05-01T14:26:31.961037Z","shell.execute_reply":"2024-05-01T14:26:31.972515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## False News Dataset","metadata":{}},{"cell_type":"markdown","source":"We preprocess the articles in a similar manner as the true dataset","metadata":{}},{"cell_type":"code","source":"# FAKE DATASET\n# Checking for NaN values\nfake_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:35.777452Z","iopub.execute_input":"2024-05-01T14:26:35.777783Z","iopub.status.idle":"2024-05-01T14:26:35.796646Z","shell.execute_reply.started":"2024-05-01T14:26:35.777758Z","shell.execute_reply":"2024-05-01T14:26:35.795731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for duplicates or blank articles\n# Conversion to dataframe to reduce output text clutter\nduplicate = fake_df['text'].value_counts()[fake_df['text'].value_counts()>1]\nduplicate = duplicate.rename_axis('unique_values').reset_index(name='counts')\nduplicate","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:36.744729Z","iopub.execute_input":"2024-05-01T14:26:36.745441Z","iopub.status.idle":"2024-05-01T14:26:36.808607Z","shell.execute_reply.started":"2024-05-01T14:26:36.74541Z","shell.execute_reply":"2024-05-01T14:26:36.807666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df['text'].value_counts()[fake_df['text'].value_counts()>1].sum() - 4927 - 626","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:38.327545Z","iopub.execute_input":"2024-05-01T14:26:38.328242Z","iopub.status.idle":"2024-05-01T14:26:38.360913Z","shell.execute_reply.started":"2024-05-01T14:26:38.328213Z","shell.execute_reply":"2024-05-01T14:26:38.360007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 4927 rows of articles that have duplicates. The total number of duplicate articles is 5401. There are 626 blank articles in the database as well.","metadata":{}},{"cell_type":"code","source":"# Values with no text - only title\nblank = fake_df.loc[fake_df[\"text\"] == duplicate[\"unique_values\"][0]]\nblank","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:39.824469Z","iopub.execute_input":"2024-05-01T14:26:39.825162Z","iopub.status.idle":"2024-05-01T14:26:39.842699Z","shell.execute_reply.started":"2024-05-01T14:26:39.825133Z","shell.execute_reply":"2024-05-01T14:26:39.841757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blank.index","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:41.376934Z","iopub.execute_input":"2024-05-01T14:26:41.377558Z","iopub.status.idle":"2024-05-01T14:26:41.383673Z","shell.execute_reply.started":"2024-05-01T14:26:41.37753Z","shell.execute_reply":"2024-05-01T14:26:41.382806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the blank news from the fake dataframe\nfake_df = fake_df.drop(blank.index)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:42.533723Z","iopub.execute_input":"2024-05-01T14:26:42.534423Z","iopub.status.idle":"2024-05-01T14:26:42.541509Z","shell.execute_reply.started":"2024-05-01T14:26:42.534392Z","shell.execute_reply":"2024-05-01T14:26:42.540512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verification of removal (verified)\nfake_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:43.396807Z","iopub.execute_input":"2024-05-01T14:26:43.397536Z","iopub.status.idle":"2024-05-01T14:26:43.409098Z","shell.execute_reply.started":"2024-05-01T14:26:43.397504Z","shell.execute_reply":"2024-05-01T14:26:43.408186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deletion of duplicates\nfake_df = fake_df.drop_duplicates(subset=['text'], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:45.481945Z","iopub.execute_input":"2024-05-01T14:26:45.482833Z","iopub.status.idle":"2024-05-01T14:26:45.492934Z","shell.execute_reply.started":"2024-05-01T14:26:45.4828Z","shell.execute_reply":"2024-05-01T14:26:45.491828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:46.314809Z","iopub.execute_input":"2024-05-01T14:26:46.315497Z","iopub.status.idle":"2024-05-01T14:26:46.327616Z","shell.execute_reply.started":"2024-05-01T14:26:46.315465Z","shell.execute_reply":"2024-05-01T14:26:46.326674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate title and text column\nfake_df['article'] = fake_df['title'] + '.' + fake_df['text']","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:48.7737Z","iopub.execute_input":"2024-05-01T14:26:48.774547Z","iopub.status.idle":"2024-05-01T14:26:48.832812Z","shell.execute_reply.started":"2024-05-01T14:26:48.774514Z","shell.execute_reply":"2024-05-01T14:26:48.83184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df = fake_df.drop(['title', 'text'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:26:52.310635Z","iopub.execute_input":"2024-05-01T14:26:52.310988Z","iopub.status.idle":"2024-05-01T14:26:52.317556Z","shell.execute_reply.started":"2024-05-01T14:26:52.31096Z","shell.execute_reply":"2024-05-01T14:26:52.316687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:00.186589Z","iopub.execute_input":"2024-05-01T14:27:00.187255Z","iopub.status.idle":"2024-05-01T14:27:00.198647Z","shell.execute_reply.started":"2024-05-01T14:27:00.187223Z","shell.execute_reply":"2024-05-01T14:27:00.197826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We process the date column once again","metadata":{}},{"cell_type":"code","source":"fake_df['date_len'] = [len(x) for x in fake_df['date']]\nprint(fake_df['date_len'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:04.477346Z","iopub.execute_input":"2024-05-01T14:27:04.478183Z","iopub.status.idle":"2024-05-01T14:27:04.496564Z","shell.execute_reply.started":"2024-05-01T14:27:04.478149Z","shell.execute_reply":"2024-05-01T14:27:04.495618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df.loc[fake_df[\"date_len\"] > 18]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:06.291575Z","iopub.execute_input":"2024-05-01T14:27:06.291926Z","iopub.status.idle":"2024-05-01T14:27:06.302909Z","shell.execute_reply.started":"2024-05-01T14:27:06.291897Z","shell.execute_reply":"2024-05-01T14:27:06.302065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df.loc[fake_df[\"date_len\"] > 18].article","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:08.118148Z","iopub.execute_input":"2024-05-01T14:27:08.118836Z","iopub.status.idle":"2024-05-01T14:27:08.126326Z","shell.execute_reply.started":"2024-05-01T14:27:08.118808Z","shell.execute_reply":"2024-05-01T14:27:08.125461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are incorrect values for dates and they are not proper articles. So they will be dropped.","metadata":{}},{"cell_type":"code","source":"bad_date = fake_df.loc[fake_df[\"date_len\"] > 18]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:11.240928Z","iopub.execute_input":"2024-05-01T14:27:11.241782Z","iopub.status.idle":"2024-05-01T14:27:11.246861Z","shell.execute_reply.started":"2024-05-01T14:27:11.241753Z","shell.execute_reply":"2024-05-01T14:27:11.24584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_date.index","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:12.529662Z","iopub.execute_input":"2024-05-01T14:27:12.530279Z","iopub.status.idle":"2024-05-01T14:27:12.535892Z","shell.execute_reply.started":"2024-05-01T14:27:12.530247Z","shell.execute_reply":"2024-05-01T14:27:12.535058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping entries with bad date and no news from the fake dataframe\nfake_df = fake_df.drop(index=bad_date.index)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:13.310558Z","iopub.execute_input":"2024-05-01T14:27:13.310912Z","iopub.status.idle":"2024-05-01T14:27:13.317853Z","shell.execute_reply.started":"2024-05-01T14:27:13.310885Z","shell.execute_reply":"2024-05-01T14:27:13.316996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verifying entries are dropped (verified)\nfake_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:14.237243Z","iopub.execute_input":"2024-05-01T14:27:14.238101Z","iopub.status.idle":"2024-05-01T14:27:14.249905Z","shell.execute_reply.started":"2024-05-01T14:27:14.238067Z","shell.execute_reply":"2024-05-01T14:27:14.248984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All data format is uniform (month date, year)\n# Unifying the date format to datetime\n# Nested try except for ValueError exceptions in the fake dataset\ndates = []\nfor x in fake_df['date']:\n    try:\n        date = datetime.strptime(x, '%B %d, %Y')\n    except ValueError:\n        try:\n            date = datetime.strptime(x, '%d-%b-%y')\n        except ValueError:\n            date = datetime.strptime(x, '%b %d, %Y')\n    dates.append(date)\nfake_df['date'] = dates","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:16.389366Z","iopub.execute_input":"2024-05-01T14:27:16.390135Z","iopub.status.idle":"2024-05-01T14:27:16.762189Z","shell.execute_reply.started":"2024-05-01T14:27:16.390097Z","shell.execute_reply":"2024-05-01T14:27:16.761332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verification\nfake_df['date'].nunique","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:17.35275Z","iopub.execute_input":"2024-05-01T14:27:17.353588Z","iopub.status.idle":"2024-05-01T14:27:17.360509Z","shell.execute_reply.started":"2024-05-01T14:27:17.353556Z","shell.execute_reply":"2024-05-01T14:27:17.359538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the dates are unique. Now the label for the fake dataset is finally added, where 0 means fake news","metadata":{}},{"cell_type":"code","source":"fake_df['label'] = 0","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:19.524662Z","iopub.execute_input":"2024-05-01T14:27:19.525433Z","iopub.status.idle":"2024-05-01T14:27:19.529933Z","shell.execute_reply.started":"2024-05-01T14:27:19.525401Z","shell.execute_reply":"2024-05-01T14:27:19.529084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df = fake_df.drop(['date_len'], axis=1)\nfake_df","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:21.119953Z","iopub.execute_input":"2024-05-01T14:27:21.120764Z","iopub.status.idle":"2024-05-01T14:27:21.137891Z","shell.execute_reply.started":"2024-05-01T14:27:21.120733Z","shell.execute_reply":"2024-05-01T14:27:21.137073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we concatenate both the dataframe","metadata":{}},{"cell_type":"code","source":"dataset = pd.concat([true_df, fake_df])\ndataset = dataset.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:24.177866Z","iopub.execute_input":"2024-05-01T14:27:24.178899Z","iopub.status.idle":"2024-05-01T14:27:24.195053Z","shell.execute_reply.started":"2024-05-01T14:27:24.178853Z","shell.execute_reply":"2024-05-01T14:27:24.194202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:27:25.233786Z","iopub.execute_input":"2024-05-01T14:27:25.234253Z","iopub.status.idle":"2024-05-01T14:27:25.246277Z","shell.execute_reply.started":"2024-05-01T14:27:25.234223Z","shell.execute_reply":"2024-05-01T14:27:25.245394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt \nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:22.997751Z","iopub.execute_input":"2024-05-01T14:28:22.998112Z","iopub.status.idle":"2024-05-01T14:28:23.272004Z","shell.execute_reply.started":"2024-05-01T14:28:22.99808Z","shell.execute_reply":"2024-05-01T14:28:23.271264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting histogram of article subjects\nfig, hist = plt.subplots(figsize = (11,7))\nhist = sns.histplot(data=dataset, x = 'subject', hue=\"label\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:24.847846Z","iopub.execute_input":"2024-05-01T14:28:24.849053Z","iopub.status.idle":"2024-05-01T14:28:25.334753Z","shell.execute_reply.started":"2024-05-01T14:28:24.848994Z","shell.execute_reply":"2024-05-01T14:28:25.333805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The subjects in fake news are different from the subjects in verified news. Subject will not be taken into account.","metadata":{}},{"cell_type":"code","source":"# Plotting histogram of article dates\nfig, hist = plt.subplots(figsize = (11,7))\nhist = sns.histplot(data=dataset, x = 'date', hue=\"label\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:27.475696Z","iopub.execute_input":"2024-05-01T14:28:27.476436Z","iopub.status.idle":"2024-05-01T14:28:28.142632Z","shell.execute_reply.started":"2024-05-01T14:28:27.476405Z","shell.execute_reply":"2024-05-01T14:28:28.141785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The date distribution of verified and false articles are varying. Fake articles span from before 2016 and verified articles are recorded from after 2016. Also, most of the verified articles in the dataset seem to be dated between Sep 2017 to Jan 2018.\nDate will not be taken into account.","metadata":{}},{"cell_type":"code","source":"# Text analysis\nav_t = dataset[dataset['label'] == 1]['article'].apply(lambda x: len(x)).mean()\nav_f = dataset[dataset['label'] == 0]['article'].apply(lambda x: len(x)).mean()\nav = pd.DataFrame(data = {'Average character length': [av_t, av_f], 'Label':['True', 'False']})\nfig, bar = plt.subplots(figsize = (11,7))\nbar = sns.barplot(y='Average character length', x='Label', data=av)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:30.236872Z","iopub.execute_input":"2024-05-01T14:28:30.237511Z","iopub.status.idle":"2024-05-01T14:28:30.499909Z","shell.execute_reply.started":"2024-05-01T14:28:30.23748Z","shell.execute_reply":"2024-05-01T14:28:30.498956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average between all fake and true news is almost the same.","metadata":{}},{"cell_type":"code","source":"# Characters length of articles\nlen_cha_true = dataset[dataset['label'] == 1]['article'].apply(lambda x: len(x))\nlen_cha_fake = dataset[dataset['label'] == 0]['article'].apply(lambda x: len(x))\n\nnorm_weights_true = np.ones(len(len_cha_true))/len(len_cha_true)\nnorm_weights_fake = np.ones(len(len_cha_fake))/len(len_cha_fake)\n\nbins = [i * 1000 for i in range(0,31)]\n\nfig, (hist1, hist2) = plt.subplots(1,2, figsize = (11,7))\nhist1.hist(len_cha_true, bins = bins, weights = norm_weights_true, color = 'C0')\nhist1.set_ylim(0, top=0.4)\nhist1.set_xlim(0, 30000)\nhist1.set_xlabel('Number of characters')\nhist1.set_ylabel('Proportion of articles')\nhist1.set_title('True texts')\n\nhist2.hist(len_cha_fake, bins = bins, weights = norm_weights_fake, color = 'C1')\nhist2.set_ylim(0, top=0.4)\nhist2.set_xlim(0, 30000)\nhist2.set_xlabel('Number of characters')\nhist2.set_ylabel('Proportion of articles')\nhist2.set_title('False texts');","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:33.778919Z","iopub.execute_input":"2024-05-01T14:28:33.779669Z","iopub.status.idle":"2024-05-01T14:28:34.445931Z","shell.execute_reply.started":"2024-05-01T14:28:33.779632Z","shell.execute_reply":"2024-05-01T14:28:34.445059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of words per article\nlen_w_true = dataset[dataset['label'] == 1]['article'].str.split().map(lambda x: len(x))\nlen_w_fake = dataset[dataset['label'] == 0]['article'].str.split().map(lambda x: len(x))\n\nnorm_weights_true = np.ones(len(len_w_true))/len(len_w_true)\nnorm_weights_fake = np.ones(len(len_w_fake))/len(len_w_fake)\n\nbins_ = [i * 200 for i in range(0,26)]\n\nfig, (hist1, hist2) = plt.subplots(1,2, figsize = (11,7))\nhist1.hist(len_w_true, bins = bins_, weights = norm_weights_true, color = 'C0')\nhist1.set_ylim(0, top=0.4)\nhist1.set_xlim(0, 5000)\nhist1.set_xlabel('Number of words / article')\nhist1.set_ylabel('Proportion of aticles')\nhist1.set_title('True texts')\nhist2.hist(len_w_fake, bins = bins_, weights = norm_weights_fake, color = 'C1')\nhist2.set_ylim(0, top=0.4)\nhist2.set_xlim(0, 5000)\nhist2.set_xlabel('Number of words / article')\nhist2.set_ylabel('Proportion of aticles')\nhist2.set_title('False texts');","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:36.474761Z","iopub.execute_input":"2024-05-01T14:28:36.475098Z","iopub.status.idle":"2024-05-01T14:28:39.30086Z","shell.execute_reply.started":"2024-05-01T14:28:36.475073Z","shell.execute_reply":"2024-05-01T14:28:39.299962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The normalized distribution of both words per article and number of characters between the two labels are somewhat similar.","metadata":{}},{"cell_type":"markdown","source":"Defining functions for preprocessing - removal of stopwords, punctuations and sanitization of web elements from the articles. (Useful for LSTM model training)","metadata":{}},{"cell_type":"code","source":"from string import punctuation\npunctuation = list(punctuation)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:41.959983Z","iopub.execute_input":"2024-05-01T14:28:41.960354Z","iopub.status.idle":"2024-05-01T14:28:41.964703Z","shell.execute_reply.started":"2024-05-01T14:28:41.960329Z","shell.execute_reply":"2024-05-01T14:28:41.963701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:43.259008Z","iopub.execute_input":"2024-05-01T14:28:43.259367Z","iopub.status.idle":"2024-05-01T14:28:43.269866Z","shell.execute_reply.started":"2024-05-01T14:28:43.25934Z","shell.execute_reply":"2024-05-01T14:28:43.268962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/remydeshayes/NLP_Pytorch/blob/main/Notebook%20-%20Fake_News%20Detection%20Pytorch%20-%20Billiot_Deshayes.ipynb\nstop = stopwords + punctuation + ['“','’', '“', '”', '‘','...']\ntqdm.pandas()\n\ndef lowerizer(article):\n  \"\"\"\n  Lowerize a given text\n  ----\n  Inputs : \n    article (str) : text to be pre-processed\n  Outputs : \n    article.lower() (str) : lowerized text\n  \"\"\"\n  return article.lower()\n\ndef remove_html(article):\n    \"\"\"\n    Remove HTML tags from a given text\n    ----\n    Inputs : \n      article (str) : text to be pre-processed\n    Outputs : \n      article (str) : text cleaned of HTML tags\n    \"\"\"\n    article = re.sub(\"(<!--.*?-->)\", \"\", article, flags=re.DOTALL)\n    return article\n\ndef remove_url(article):\n    \"\"\"\n    Remove URL tags from a given text\n    ----\n    Inputs : \n      article (str) : text to be pre-processed\n    Outputs : \n      article (str) : text cleaned of URL tags\n    \"\"\"\n    article = re.sub(r'https?:\\/\\/.\\S+', \"\", article)\n    return article\n\ndef remove_hashtags(article):\n    \"\"\"\n    Remove hashtags from a given text\n    ----\n    Inputs : \n      article (str) : text to be pre-processed\n    Outputs : \n      article (str) : text cleaned of hashtags\n    \"\"\"\n    article = re.sub(\"#\",\" \",article)\n    return article\n\ndef remove_a(article):\n    \"\"\"\n    Remove twitter account references @ rom a given text\n    ----\n    Inputs : \n      article (str) : text to be pre-processed\n    Outputs : \n      article (str) : text withouttwitter account references \n    \"\"\"\n    article = re.sub(\"@\",\" \",article)\n    return article\n\ndef remove_brackets(article):\n    \"\"\"\n    Remove square brackets from a given text \n    ----\n    Inputs : \n      article (str) : text to be pre-processed\n    Outputs : \n      article (str) : text without square brackets\n    \"\"\"\n    article = re.sub('\\[[^]]*\\]', '', article)\n    return article\n\ndef remove_stop_punct(article):\n    \"\"\"\n    Remove punctuation and stopwords from a given text\n    ----\n    Inputs : \n      article (str) : text to be pre-processed\n    Outputs : \n      article (str) : text without punctuation or stopwords\n    \"\"\"\n    final_article = []\n    for i in article.split():\n        if i not in stop:\n            final_article.append(i.strip())\n    return \" \".join(final_article)\n\ndef preprocessing(article, lowerizer_=False, remove_web=True, remove_brackets_=False, remove_stop_punct_=False):\n    \"\"\"\n    Computes the above-define steps to clean a given text\n    ----\n    Inputs : \n      article (str) : text to be pre-processed\n    Outputs : \n      article (str) : pre-processed text\n    \"\"\"\n    \n    if lowerizer_:\n        article = lowerizer(article)\n    if remove_web:\n        article = remove_html(article)\n        article = remove_url(article)\n        article = remove_hashtags(article)\n        article = remove_a(article)\n    if remove_brackets_:\n        article = remove_brackets(article)\n    if remove_stop_punct_:\n        article = remove_stop_punct(article)\n    return article","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:45.054167Z","iopub.execute_input":"2024-05-01T14:28:45.054489Z","iopub.status.idle":"2024-05-01T14:28:45.068537Z","shell.execute_reply.started":"2024-05-01T14:28:45.054466Z","shell.execute_reply":"2024-05-01T14:28:45.067668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['article_lstm'] = dataset['article'].progress_apply(\n    lambda x : preprocessing(x, lowerizer_=True, remove_brackets_=True, remove_stop_punct_=True))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:28:48.286687Z","iopub.execute_input":"2024-05-01T14:28:48.287544Z","iopub.status.idle":"2024-05-01T14:29:26.940151Z","shell.execute_reply.started":"2024-05-01T14:28:48.287511Z","shell.execute_reply":"2024-05-01T14:29:26.939239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:29:29.009147Z","iopub.execute_input":"2024-05-01T14:29:29.009846Z","iopub.status.idle":"2024-05-01T14:29:29.023783Z","shell.execute_reply.started":"2024-05-01T14:29:29.009815Z","shell.execute_reply":"2024-05-01T14:29:29.022797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model prediction","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# Declaring constants\nSEED = 42\nMAX_LENGTH = 100","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:29:31.46239Z","iopub.execute_input":"2024-05-01T14:29:31.463314Z","iopub.status.idle":"2024-05-01T14:29:31.467518Z","shell.execute_reply.started":"2024-05-01T14:29:31.463272Z","shell.execute_reply":"2024-05-01T14:29:31.466664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We shuffle the dataset and then split the dataset into training, validation and test datasets.","metadata":{}},{"cell_type":"code","source":"# Shuffling the dataset\nshuffled_data = dataset.sample(frac=1, random_state=SEED).reset_index(drop=True)\nshuffled_data","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:29:33.001427Z","iopub.execute_input":"2024-05-01T14:29:33.00176Z","iopub.status.idle":"2024-05-01T14:29:33.036681Z","shell.execute_reply.started":"2024-05-01T14:29:33.001736Z","shell.execute_reply":"2024-05-01T14:29:33.035777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:29:42.342527Z","iopub.execute_input":"2024-05-01T14:29:42.343343Z","iopub.status.idle":"2024-05-01T14:29:42.347244Z","shell.execute_reply.started":"2024-05-01T14:29:42.343312Z","shell.execute_reply":"2024-05-01T14:29:42.346356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = shuffled_data['article_lstm']\ny = shuffled_data['label']","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:29:45.028529Z","iopub.execute_input":"2024-05-01T14:29:45.029331Z","iopub.status.idle":"2024-05-01T14:29:45.033463Z","shell.execute_reply.started":"2024-05-01T14:29:45.0293Z","shell.execute_reply":"2024-05-01T14:29:45.032461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train-Validation-Test set split into 80:10:10 ratio\ntrain_X, temp_X, train_y, temp_y = train_test_split(X, y, random_state=SEED, test_size=0.2, stratify=y)\n# Validation-Test split\nvalid_X, test_X, valid_y, test_y = train_test_split(temp_X, temp_y, random_state=SEED, test_size=0.5, stratify=temp_y)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:29:48.852Z","iopub.execute_input":"2024-05-01T14:29:48.852377Z","iopub.status.idle":"2024-05-01T14:29:48.879383Z","shell.execute_reply.started":"2024-05-01T14:29:48.85235Z","shell.execute_reply":"2024-05-01T14:29:48.878562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Return size of the split datasets\n(len(train_X), len(train_y)), (len(valid_X), len(valid_y)), (len(test_X), len(test_y))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:29:49.954287Z","iopub.execute_input":"2024-05-01T14:29:49.955126Z","iopub.status.idle":"2024-05-01T14:29:49.961426Z","shell.execute_reply.started":"2024-05-01T14:29:49.955096Z","shell.execute_reply":"2024-05-01T14:29:49.960531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to ascertain balance of true and fake news\n# https://github.com/remydeshayes/NLP_Pytorch/blob/main/Notebook%20-%20Fake_News%20Detection%20Pytorch%20-%20Billiot_Deshayes.ipynb\ndef distribution_data(corpus): \n    \"\"\"\n    Returns number of fake and true news in a given dataset\n    ----\n    Inputs : \n    corpus (array) : labels of our dataset\n    Outputs : \n    distrib (pd.DataFrame) : number of true and fake news in the dataset \n    \"\"\"\n    nb_true = corpus.sum()\n    nb_false = len(corpus) - nb_true\n    distrib = pd.DataFrame(data = {'Number of samples': [nb_true, nb_false], 'Label':['True', 'False']})\n    return distrib","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:30:55.898683Z","iopub.execute_input":"2024-04-16T06:30:55.899202Z","iopub.status.idle":"2024-04-16T06:30:55.906356Z","shell.execute_reply.started":"2024-04-16T06:30:55.899161Z","shell.execute_reply":"2024-04-16T06:30:55.9051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distrib = distribution_data(dataset['label'])\nfig, bar = plt.subplots(figsize = (2,4))\nbar = sns.barplot(y='Number of samples', x='Label',data=distrib);","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:30:58.044579Z","iopub.execute_input":"2024-04-16T06:30:58.045047Z","iopub.status.idle":"2024-04-16T06:30:58.306545Z","shell.execute_reply.started":"2024-04-16T06:30:58.045013Z","shell.execute_reply":"2024-04-16T06:30:58.305432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more Validated news than Falsified news in the dataset, but ideal for the model to be trained.","metadata":{}},{"cell_type":"code","source":"train_distrib = distribution_data(train_y)\nvalid_distrib = distribution_data(valid_y)\ntest_distrib = distribution_data(test_y)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:31:01.060025Z","iopub.execute_input":"2024-04-16T06:31:01.060534Z","iopub.status.idle":"2024-04-16T06:31:01.069234Z","shell.execute_reply.started":"2024-04-16T06:31:01.060492Z","shell.execute_reply":"2024-04-16T06:31:01.067873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (bar1, bar2, bar3) = plt.subplots(1,3, figsize = (11,7))\nsns.barplot(y='Number of samples', x='Label',data=train_distrib, ax = bar1)\nsns.barplot(y='Number of samples', x='Label',data=valid_distrib, ax = bar2)\nsns.barplot(y='Number of samples', x='Label',data=test_distrib, ax = bar3)\nbar1.set_title(\"Training\")\nbar2.set_title(\"Validation\")\nbar3.set_title(\"Testing\");","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:31:05.227013Z","iopub.execute_input":"2024-04-16T06:31:05.2275Z","iopub.status.idle":"2024-04-16T06:31:06.029819Z","shell.execute_reply.started":"2024-04-16T06:31:05.227461Z","shell.execute_reply":"2024-04-16T06:31:06.028596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same can be said for the shuffled and split dataset. They share the same ratio of true to fake news.","metadata":{}},{"cell_type":"markdown","source":"A class is created that will build vocabulary from the training dataset and create a dictionary - indexes to words and words to indexes.  \nThe clean article is processed by tokenizing using nltk, transposed to indexes, passed as tensor and padding to a determined length.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:31:11.716763Z","iopub.execute_input":"2024-04-16T06:31:11.718232Z","iopub.status.idle":"2024-04-16T06:31:16.212904Z","shell.execute_reply.started":"2024-04-16T06:31:11.718169Z","shell.execute_reply":"2024-04-16T06:31:16.211647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing nltk (yet again)\nimport nltk\nfrom nltk import word_tokenize\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:31:20.014221Z","iopub.execute_input":"2024-04-16T06:31:20.015308Z","iopub.status.idle":"2024-04-16T06:31:20.11053Z","shell.execute_reply.started":"2024-04-16T06:31:20.015267Z","shell.execute_reply":"2024-04-16T06:31:20.109178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/remydeshayes/NLP_Pytorch/blob/main/Notebook%20-%20Fake_News%20Detection%20Pytorch%20-%20Billiot_Deshayes.ipynb\n# Definition of class that will preprocess the words to indexes, tranform to tensor and cut/pad to the desired sequence of length\n# These functions are automated when using HuggingFace tokenizers\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, data, categories, vocab = None, max_length = 100, min_freq = 5):\n        \n        self.data = data\n        self.max_length = max_length\n        \n        # Allow to import a vocabulary (validation and testing will use the training vocabulary)\n        if vocab is not None:\n            self.word2idx, self.idx2word = vocab\n        else:\n            # Build the vocabulary if none is imported\n            self.word2idx, self.idx2word = self.build_vocab(self.data, min_freq)\n        \n        # We tokenize the articles\n        tokenized_data = [word_tokenize(file.lower()) for file in self.data]\n        # Transform words into lists of indexes\n        indexed_data = [[self.word2idx.get(word, self.word2idx['UNK']) for word in file] for file in tokenized_data]\n        # Transform into a list of Pytorch LongTensors\n        tensor_data = [torch.LongTensor(file) for file in indexed_data]\n        # Lables are passed into a FloatTensor (adding to_numpy() to resolve ValueError)\n        # ValueError: could not determine the shape of object type 'Series'\n        tensor_y = torch.FloatTensor(categories.to_numpy())\n        # Finally we cut too the determined maximum length\n        cut_tensor_data = [tensor[:max_length] for tensor in tensor_data]\n        # We pad the sequences to have the whole dataset containing sequences of the same length\n        self.tensor_data = pad_sequence(cut_tensor_data, batch_first=True, padding_value=0)\n        self.tensor_y = tensor_y\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        return self.tensor_data[idx], self.tensor_y[idx] \n    \n    def build_vocab(self, corpus, count_threshold):\n        word_counts = {}\n        for sent in tqdm(corpus):\n            for word in word_tokenize(sent.lower()):\n                if word not in word_counts:\n                    word_counts[word] = 0\n                word_counts[word] += 1   \n        filtered_word_counts = {word: count for word, count in word_counts.items() if count >= count_threshold}        \n        words = sorted(filtered_word_counts.keys(), key=word_counts.get, reverse=True) + ['UNK']\n        word_index = {words[i] : (i+1) for i in range(len(words))}\n        idx_word = {(i+1) : words[i] for i in range(len(words))}\n        return word_index, idx_word\n    \n    def get_vocab(self):\n        return self.word2idx, self.idx2word","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:31:23.975654Z","iopub.execute_input":"2024-04-16T06:31:23.976459Z","iopub.status.idle":"2024-04-16T06:31:23.997217Z","shell.execute_reply.started":"2024-04-16T06:31:23.97642Z","shell.execute_reply":"2024-04-16T06:31:23.995826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start preprocessing the training data, and after building the vocabulary, it is used to prepare the validation and testing dataset.","metadata":{}},{"cell_type":"code","source":"training_dataset = TextClassificationDataset(train_X, train_y, max_length=MAX_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:31:31.159244Z","iopub.execute_input":"2024-04-16T06:31:31.159756Z","iopub.status.idle":"2024-04-16T06:36:49.864118Z","shell.execute_reply.started":"2024-04-16T06:31:31.159699Z","shell.execute_reply":"2024-04-16T06:36:49.862809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_word2idx, training_idx2word = training_dataset.get_vocab()\nvalid_dataset = TextClassificationDataset(valid_X, valid_y, (training_word2idx, training_idx2word), max_length=MAX_LENGTH)\ntest_dataset = TextClassificationDataset(test_X, test_y, (training_word2idx, training_idx2word), max_length=MAX_LENGTH)","metadata":{"execution":{"iopub.status.idle":"2024-04-16T06:38:05.335565Z","shell.execute_reply.started":"2024-04-16T06:37:27.553996Z","shell.execute_reply":"2024-04-16T06:38:05.334237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The datasets are then passed into a DataLoader","metadata":{}},{"cell_type":"code","source":"training_dataloader = DataLoader(training_dataset, batch_size = 200, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size = 25)\ntest_dataloader = DataLoader(test_dataset, batch_size = 25)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:49:28.370874Z","iopub.execute_input":"2024-04-16T06:49:28.371414Z","iopub.status.idle":"2024-04-16T06:49:28.378457Z","shell.execute_reply.started":"2024-04-16T06:49:28.371374Z","shell.execute_reply":"2024-04-16T06:49:28.377312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embeddings and Model Definition","metadata":{}},{"cell_type":"markdown","source":"A pretrained GloVe embedding with 300 features per word vector is loaded, which had been trained on Wikipedia.  \nGloVe leverages global word to word co-occurance counts, in contrast to Word2vec, which leverages co-occurance within neighboring words.","metadata":{}},{"cell_type":"code","source":"import gensim.downloader as api\nloaded_glove_model = api.load(\"glove-wiki-gigaword-300\")\nloaded_glove_embeddings = loaded_glove_model.vectors","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:49:33.134687Z","iopub.execute_input":"2024-04-16T06:49:33.135186Z","iopub.status.idle":"2024-04-16T06:53:11.399559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After creation of the word to index dictionary, it is passed into the GloVe embedding function. The internal working is commented in the function definition","metadata":{}},{"cell_type":"code","source":"# https://github.com/remydeshayes/NLP_Pytorch/blob/main/Notebook%20-%20Fake_News%20Detection%20Pytorch%20-%20Billiot_Deshayes.ipynb\n\ndef get_glove_adapted_embeddings(glove_model, input_voc):\n    \"\"\"\n    Retrieve a vocabulary words'embeddings from GloVe \n    ----\n    Inputs : \n    glove_model () : GloVe Embedding model\n    input_voc (dict) : dictionnary of our indexed vocabulary \n    Outputs : \n    embeddings (ndarray) : GloVe Embeddings for the given vocabulary with the vocabulary index\n    \"\"\"\n    # Create a dict: Get the corresponding GloVe vocabulary from the word index\n    keys = {i: glove_model.key_to_index[w] if w in glove_model.key_to_index.keys() else None for w, i in input_voc.items()}\n#     print(keys)\n    # Create a dict of index corresponding to index of the word in GloVe\n    index_dict = {i: key for i, key in keys.items() if key is not None}\n    # Create a matrix to retrieve GloVe vectors\n    embeddings = np.zeros((len(input_voc)+1,glove_model.vectors.shape[1]))\n    # Populate the matrix with the vectors and return this matrix\n    for i, ind in index_dict.items():\n        embeddings[i] = glove_model.vectors[ind]\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:56:44.56219Z","iopub.execute_input":"2024-04-16T06:56:44.562658Z","iopub.status.idle":"2024-04-16T06:56:44.572521Z","shell.execute_reply.started":"2024-04-16T06:56:44.562624Z","shell.execute_reply":"2024-04-16T06:56:44.571271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, training_word2idx)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:56:55.915184Z","iopub.execute_input":"2024-04-16T06:56:55.915673Z","iopub.status.idle":"2024-04-16T06:56:56.167402Z","shell.execute_reply.started":"2024-04-16T06:56:55.915637Z","shell.execute_reply":"2024-04-16T06:56:56.166088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training model is now defined: an embedding layer of dim(vocab_size, 300), two LSTM layers with hidden_size 256, followed by GloVe vectors as embeddings and a fully connected layer.","metadata":{}},{"cell_type":"code","source":"class LSTMModel(nn.Module):\n\n    def __init__(self, embedding_dim, vocabulary_size, hidden_dim, embeddings=None, fine_tuning=False):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        if embeddings:\n            self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(GloveEmbeddings), freeze=not fine_tuning, padding_idx=0)\n        else:\n            self.embeddings = nn.Embedding(num_embeddings=vocabulary_size+1, embedding_dim=embedding_dim, padding_idx=0)\n\n        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, num_layers=2)\n        self.linear = nn.Linear(in_features=2*hidden_dim, out_features=1)\n\n    def forward(self, inputs):\n        emb = self.embeddings(inputs)\n        lstm_out, (ht, ct) = self.lstm(emb, None)\n        h = torch.cat((ht[-2], ht[-1]), dim=1)\n        x = torch.squeeze(self.linear(h))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:57:01.444402Z","iopub.execute_input":"2024-04-16T06:57:01.44609Z","iopub.status.idle":"2024-04-16T06:57:01.469418Z","shell.execute_reply.started":"2024-04-16T06:57:01.44604Z","shell.execute_reply":"2024-04-16T06:57:01.467964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"Three functions are defined: `train_epoch` which iterates through the training DataLoader object and computes a complete pass for every batch, `eval_model` which evaluates throught the validation/testing dataset only a forward pass for every batch and `experiment` which iterates through number of epochs for the entire training session.","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, opt, criterion, dataloader):\n    \"\"\"\n    Trains the mode over an epoch \n    ----\n    Inputs : \n    model () : defined model to be trained\n    opt () : chosen and defined optimizer \n    criterion () : chosen and defined loss\n    dataloader() : iterable object with the batches\n    Outputs : \n    losses (list) : list of training loss for each batch of the epoch\n    accs (list) : list of training accuracy for each batch of the epoch\n    \"\"\"\n    model.train()\n    losses = []\n    accs = []\n    for i, (x, y) in enumerate(dataloader):\n        opt.zero_grad()\n        # Forward pass\n        pred = model(x)\n        # Loss Computation\n        loss = criterion(pred, y)\n        # Backward pass\n        loss.backward()\n        # Weights update\n        opt.step()\n        losses.append(loss.item())\n        # Compute accuracy\n        num_corrects = sum((torch.sigmoid(pred)>0.5) == y)\n        acc = 100.0 * num_corrects/len(y)\n        accs.append(acc.item())\n        if (i%20 == 0):\n            print(\"Batch \" + str(i) + \" : training loss = \" + str(loss.item()) + \"; training acc = \" + str(acc.item()))\n    return losses, accs","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:57:12.861013Z","iopub.execute_input":"2024-04-16T06:57:12.8616Z","iopub.status.idle":"2024-04-16T06:57:12.873112Z","shell.execute_reply.started":"2024-04-16T06:57:12.861558Z","shell.execute_reply":"2024-04-16T06:57:12.871627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, criterion, evalloader):\n    \"\"\"\n    Evaluate the model  \n    ----\n    Inputs : \n    model () : defined model to be trained\n    criterion () : chosen and defined loss\n    evalloader() : iterable object with the batches \n    Outputs : \n    total_epoch_loss/(i+1) (float) : computed loss \n    total_epoch_acc/(i+1) (float) : computed accuracy \n    preds (list) : predictions made by the model\n    \"\"\"\n    model.eval()\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    preds = []\n    with torch.no_grad():\n        for i, (x, y) in enumerate(evalloader):\n            pred = model(x)\n            loss = criterion(pred, y)\n            num_corrects = sum((torch.sigmoid(pred)>0.5) == y)\n            acc = 100.0 * num_corrects/len(y)\n            total_epoch_loss += loss.item()\n            total_epoch_acc += acc.item()\n            preds.append(pred)\n\n    return total_epoch_loss/(i+1), total_epoch_acc/(i+1), preds","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:57:15.282547Z","iopub.execute_input":"2024-04-16T06:57:15.283272Z","iopub.status.idle":"2024-04-16T06:57:15.292967Z","shell.execute_reply.started":"2024-04-16T06:57:15.283236Z","shell.execute_reply":"2024-04-16T06:57:15.291347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def experiment(model, opt, criterion, num_epochs = 5):\n    \"\"\"\n    Trains & Evaluates the model over all epochs \n    ----\n    Inputs : \n    model () : defined model to be trained\n    opt () : chosen and defined optimizer \n    criterion () : chosen and defined loss\n    num_epochs() : chosen number of epochs to go through\n    Outputs : \n    train_losses (list): training losses of all batches for each epochs\n    valid_losses (list): losses over vaidation data for all epochs\n    test_loss (list): loss over test data once the model is trained \n    train_accs (list): training accuracies of all batches for each epochs\n    valid_accs (list): accuracies over vaidation data for all epochs\n    test_acc (list): accuracy over test data once the model is trained\n    test_preds (): predictions on test dataset\n    \"\"\"\n    train_losses = []\n    valid_losses = []\n    train_accs = []\n    valid_accs = []\n    print(\"Beginning training...\")\n    for e in range(num_epochs):\n        print(\"Epoch \" + str(e+1) + \":\")\n        losses, accs = train_epoch(model, opt, criterion, training_dataloader)\n        train_losses.append(losses)\n        train_accs.append(accs)\n        valid_loss, valid_acc, val_preds = eval_model(model, criterion, valid_dataloader)\n        valid_losses.append(valid_loss)\n        valid_accs.append(valid_acc)\n        print(\"Epoch \" + str(e+1) + \" : Validation loss = \" + str(valid_loss) + \"; Validation acc = \" + str(valid_acc))\n    test_loss, test_acc, test_preds = eval_model(model, criterion, test_dataloader)\n    print(\"Test loss = \" + str(test_loss) + \"; Test acc = \" + str(test_acc))\n    return train_losses, valid_losses, test_loss, train_accs, valid_accs, test_acc, test_preds","metadata":{"execution":{"iopub.status.busy":"2024-04-16T06:57:25.844791Z","iopub.execute_input":"2024-04-16T06:57:25.845644Z","iopub.status.idle":"2024-04-16T06:57:25.858598Z","shell.execute_reply.started":"2024-04-16T06:57:25.845604Z","shell.execute_reply":"2024-04-16T06:57:25.85704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training with LSTM Model","metadata":{}},{"cell_type":"markdown","source":"The model is defined, the optimizer used is Adam and the loss function used is binary cross entropy - since the task is binary classification.","metadata":{}},{"cell_type":"code","source":"# Setting the hyperparameters of the model\nEMBEDDING_DIM = 300\nVOCAB_SIZE = len(training_word2idx)\nHIDDEN_DIM = 256\nlearning_rate = 0.0025\nnum_epochs = 10","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:42:03.454313Z","iopub.execute_input":"2024-04-16T07:42:03.455499Z","iopub.status.idle":"2024-04-16T07:42:03.460566Z","shell.execute_reply.started":"2024-04-16T07:42:03.455458Z","shell.execute_reply":"2024-04-16T07:42:03.459409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lstm = LSTMModel(EMBEDDING_DIM, VOCAB_SIZE, HIDDEN_DIM,  embeddings=True, fine_tuning=False)\nopt = optim.Adam(model_lstm.parameters(), lr=learning_rate, betas=(0.9, 0.999))\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:42:07.27691Z","iopub.execute_input":"2024-04-16T07:42:07.2774Z","iopub.status.idle":"2024-04-16T07:42:07.335586Z","shell.execute_reply.started":"2024-04-16T07:42:07.277364Z","shell.execute_reply":"2024-04-16T07:42:07.333935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses_lstm, valid_losses_lstm, test_loss_lstm, train_accs_lstm, valid_accs_lstm, test_acc_lstm, test_preds_lstm = experiment(model_lstm, opt, criterion, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T07:42:11.43953Z","iopub.execute_input":"2024-04-16T07:42:11.440001Z","iopub.status.idle":"2024-04-16T08:31:28.270687Z","shell.execute_reply.started":"2024-04-16T07:42:11.439961Z","shell.execute_reply":"2024-04-16T08:31:28.26915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test accuracy of 99.63% is quite a high metric so the model weights are saved.","metadata":{}},{"cell_type":"code","source":"torch.save(model_lstm, '/kaggle/working/lstm_saved_weights.pt')\ntorch.save(model_lstm.state_dict(), '/kaggle/working/lstm_state_dict.pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:39:07.590241Z","iopub.execute_input":"2024-04-16T09:39:07.590894Z","iopub.status.idle":"2024-04-16T09:39:07.856524Z","shell.execute_reply.started":"2024-04-16T09:39:07.590849Z","shell.execute_reply":"2024-04-16T09:39:07.855385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training and validation accuracy is plotted against the number of epochs","metadata":{}},{"cell_type":"code","source":"import statistics\nfrom statistics import mean","metadata":{"execution":{"iopub.status.busy":"2024-04-16T08:33:32.524242Z","iopub.execute_input":"2024-04-16T08:33:32.525111Z","iopub.status.idle":"2024-04-16T08:33:32.531706Z","shell.execute_reply.started":"2024-04-16T08:33:32.525063Z","shell.execute_reply":"2024-04-16T08:33:32.530327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses = [mean(train_loss) for train_loss in train_losses_lstm]\ntrain_accs = [mean(train_acc) for train_acc in train_accs_lstm]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T08:33:38.091118Z","iopub.execute_input":"2024-04-16T08:33:38.092399Z","iopub.status.idle":"2024-04-16T08:33:38.103196Z","shell.execute_reply.started":"2024-04-16T08:33:38.092352Z","shell.execute_reply":"2024-04-16T08:33:38.101442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = [i for i in range(num_epochs)]\nfig , ax = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_accs , 'C0o-' , label = 'Training Accuracy (LSTM)')\nax[0].plot(epochs , valid_accs_lstm , 'C1o-' , label = 'validation Accuracy (LSTM)')\nax[0].set_title('Training & Validation Accuracy (LSTM)')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_losses , 'C0o-' , label = 'Training Loss (LSTM)')\nax[1].plot(epochs , valid_losses_lstm , 'C1o-' , label = 'Validation Loss (LSTM)')\nax[1].set_title('Training & Validation Loss (LSTM)')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T08:33:41.537361Z","iopub.execute_input":"2024-04-16T08:33:41.53787Z","iopub.status.idle":"2024-04-16T08:33:42.223405Z","shell.execute_reply.started":"2024-04-16T08:33:41.537831Z","shell.execute_reply":"2024-04-16T08:33:42.221605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training with CNN","metadata":{}},{"cell_type":"markdown","source":"Definition of a CNN model to establish performance comparison versus LSTM. The CNN model has a similar embedding layer with size (vocab_size, embedding_dim=300), a CNN layer with size (embedding_size = 300, 64, 16) with ReLU activation function, max-pooling layer, dropout layer set to 0.5 and a fully connected layer (64, 1)","metadata":{}},{"cell_type":"code","source":"from torch.nn import functional as F\n\nclass CNNModel(nn.Module):\n    def __init__(self, embedding_dim, vocabulary_size, window_size: int = 16, filter_multiplier = 64, embeddings = None, fine_tuning = False):\n        super().__init__()\n        self.embedding_dim = embedding_dim \n        if embeddings:\n            self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(GloveEmbeddings), freeze=not fine_tuning, padding_idx=0)\n        else:\n            self.embeddings = nn.Embedding(num_embeddings=vocabulary_size+1, embedding_dim=embedding_dim, padding_idx=0)\n\n        self.conv1d = nn.Conv1d(embedding_dim, filter_multiplier, window_size)\n        self.dropout = nn.Dropout(0.5)\n        self.linear = nn.Linear(filter_multiplier, 1)\n\n    def forward(self, inputs):\n        x = self.embeddings(inputs)\n        x = x.permute(0, 2, 1)\n        x = self.conv1d(x)\n        x = F.relu(x)\n        x = F.max_pool1d(x, x.shape[2]).squeeze(2)\n        x = self.dropout(x)\n        output = torch.squeeze(self.linear(x))\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-04-16T08:33:55.736505Z","iopub.execute_input":"2024-04-16T08:33:55.739097Z","iopub.status.idle":"2024-04-16T08:33:55.751193Z","shell.execute_reply.started":"2024-04-16T08:33:55.739046Z","shell.execute_reply":"2024-04-16T08:33:55.749815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Adam optimizer and Binary Cross Entropy Loss criterion is kept the same as before.","metadata":{}},{"cell_type":"code","source":"model_cnn = CNNModel(300, len(training_word2idx), 16, 64, embeddings = True, fine_tuning=True)\noptimizer = optim.Adam(model_cnn.parameters(), lr=0.0025, betas=(0.9, 0.999))\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T08:34:00.638622Z","iopub.execute_input":"2024-04-16T08:34:00.639152Z","iopub.status.idle":"2024-04-16T08:34:00.764667Z","shell.execute_reply.started":"2024-04-16T08:34:00.639114Z","shell.execute_reply":"2024-04-16T08:34:00.76333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses_cnn, valid_losses_cnn, test_loss_cnn, train_accs_cnn, valid_accs_cnn, test_acc_cnn, test_preds_cnn = experiment(model_cnn, optimizer, criterion, num_epochs=7)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T08:34:04.921877Z","iopub.execute_input":"2024-04-16T08:34:04.922497Z","iopub.status.idle":"2024-04-16T08:47:59.390305Z","shell.execute_reply.started":"2024-04-16T08:34:04.922451Z","shell.execute_reply":"2024-04-16T08:47:59.388877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses_cnn = [mean(train_loss) for train_loss in train_losses_cnn]\ntrain_accs_cnn = [mean(train_acc) for train_acc in train_accs_cnn]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:01:38.07555Z","iopub.execute_input":"2024-04-16T09:01:38.076066Z","iopub.status.idle":"2024-04-16T09:01:38.086665Z","shell.execute_reply.started":"2024-04-16T09:01:38.076032Z","shell.execute_reply":"2024-04-16T09:01:38.084987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = [i for i in range(7)] # epochs is 7\nfig , ax = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_accs_cnn , 'C0o-' , label = 'Training Accuracy (CNN)')\nax[0].plot(epochs , valid_accs_cnn , 'C1o-' , label = 'validation Accuracy (CNN)')\nax[0].set_title('Training & Validation Accuracy (CNN)')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_losses_cnn , 'C0o-' , label = 'Training Loss (CNN)')\nax[1].plot(epochs , valid_losses_cnn , 'C1o-' , label = 'Validation Loss (CNN)')\nax[1].set_title('Training & Validation Loss (CNN)')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:01:42.763274Z","iopub.execute_input":"2024-04-16T09:01:42.763811Z","iopub.status.idle":"2024-04-16T09:01:43.493382Z","shell.execute_reply.started":"2024-04-16T09:01:42.76377Z","shell.execute_reply":"2024-04-16T09:01:43.492133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model has overfitted to the training data, but the CNN model is unable to converge closer to the training accuracy metric, compared to the LSTM model.","metadata":{}},{"cell_type":"markdown","source":"# Evaluation of LSTM Model","metadata":{}},{"cell_type":"markdown","source":"The LSTM model is evaluated using confusion matrix and classification report metrics.","metadata":{}},{"cell_type":"code","source":"preds = [(torch.sigmoid(t)>0.5).tolist() for t in test_preds_lstm]\npreds = [int(t) for el in preds for t in el]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:04:50.544859Z","iopub.execute_input":"2024-04-16T09:04:50.545361Z","iopub.status.idle":"2024-04-16T09:04:50.557424Z","shell.execute_reply.started":"2024-04-16T09:04:50.545324Z","shell.execute_reply":"2024-04-16T09:04:50.555947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions\npreds[:20]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:04:52.628156Z","iopub.execute_input":"2024-04-16T09:04:52.628649Z","iopub.status.idle":"2024-04-16T09:04:52.638207Z","shell.execute_reply.started":"2024-04-16T09:04:52.628611Z","shell.execute_reply":"2024-04-16T09:04:52.636852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ground Truth\ntest_y[:20]","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:04:54.999167Z","iopub.execute_input":"2024-04-16T09:04:54.999637Z","iopub.status.idle":"2024-04-16T09:04:55.011321Z","shell.execute_reply.started":"2024-04-16T09:04:54.999603Z","shell.execute_reply":"2024-04-16T09:04:55.00975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test_y, preds)\ncm_ = pd.DataFrame(cm , index = ['True','Fake'] , columns = ['True','Fake'])","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:05:23.855205Z","iopub.execute_input":"2024-04-16T09:05:23.855816Z","iopub.status.idle":"2024-04-16T09:05:23.869863Z","shell.execute_reply.started":"2024-04-16T09:05:23.855771Z","shell.execute_reply":"2024-04-16T09:05:23.868388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (6,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['True','Fake'] , yticklabels = ['True','Fake'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\");","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:12:20.179464Z","iopub.execute_input":"2024-04-16T09:12:20.17996Z","iopub.status.idle":"2024-04-16T09:12:20.536553Z","shell.execute_reply.started":"2024-04-16T09:12:20.179924Z","shell.execute_reply":"2024-04-16T09:12:20.535126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (6,6))\nsns.heatmap(cm/3864*100,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['True','Fake'] , yticklabels = ['True','Fake'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\");","metadata":{"execution":{"iopub.status.busy":"2024-04-30T10:17:19.823205Z","iopub.execute_input":"2024-04-30T10:17:19.824072Z","iopub.status.idle":"2024-04-30T10:17:20.132343Z","shell.execute_reply.started":"2024-04-30T10:17:19.824039Z","shell.execute_reply":"2024-04-30T10:17:20.131377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(test_y, preds, target_names = ['Predicted Fake','Predicted True']))","metadata":{"execution":{"iopub.status.busy":"2024-04-16T09:12:23.47934Z","iopub.execute_input":"2024-04-16T09:12:23.479958Z","iopub.status.idle":"2024-04-16T09:12:23.50592Z","shell.execute_reply.started":"2024-04-16T09:12:23.479912Z","shell.execute_reply":"2024-04-16T09:12:23.504539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thoughts","metadata":{}},{"cell_type":"markdown","source":"The binary classification of detecting fake news can be solved nowadays using Transformers. While using the LSTM and the CNN models to train, GloVe embeddings were utilized to convert words into their corresponding vectors. GloVe leverages global word-to-word co-occurences which effect the final word vector. This is comparable to the how the attention layers of the Transformer models are able to understand word context in a given passage of text.  \nIt could be said that while the CNN model came close to the verification accuracy, the consequences of overfitting made it a poor architecture when compared to the LSTM model. An RNN would therefore have a better performance index than the CNN model. It would however run into the problem of vanishing or exploding gradients. Since LSTM solves this problem, it would be unwise to reinvent the wheel.","metadata":{}},{"cell_type":"markdown","source":"# Training on BERT","metadata":{}},{"cell_type":"markdown","source":"Preprocessing the data suitable for BERT tokenizer and BERT Transformer model. As most of the preprocessing has already been done, some of the steps would be straightforward.","metadata":{}},{"cell_type":"code","source":"shuffled_data['article_bert'] = shuffled_data['article'].progress_apply(\n    lambda x : preprocessing(x, lowerizer_=False, remove_web=True, remove_brackets_=False, remove_stop_punct_=False))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:31:13.514168Z","iopub.execute_input":"2024-05-01T14:31:13.514523Z","iopub.status.idle":"2024-05-01T14:31:14.28799Z","shell.execute_reply.started":"2024-05-01T14:31:13.514496Z","shell.execute_reply":"2024-05-01T14:31:14.28713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shuffled_data","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:31:15.975906Z","iopub.execute_input":"2024-05-01T14:31:15.976261Z","iopub.status.idle":"2024-05-01T14:31:15.990624Z","shell.execute_reply.started":"2024-05-01T14:31:15.976233Z","shell.execute_reply":"2024-05-01T14:31:15.989725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the training dataset","metadata":{}},{"cell_type":"code","source":"bert_X = shuffled_data['article_bert']\nbert_y = shuffled_data['label']","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:31:19.701932Z","iopub.execute_input":"2024-05-01T14:31:19.702278Z","iopub.status.idle":"2024-05-01T14:31:19.706728Z","shell.execute_reply.started":"2024-05-01T14:31:19.702253Z","shell.execute_reply":"2024-05-01T14:31:19.705772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train-Validation-Test set split into 80:10:10 ratio\nb_train_X, b_temp_X, b_train_y, b_temp_y = train_test_split(bert_X, bert_y, random_state=SEED, test_size=0.2, stratify=y)\n# Validation-Test split\nb_valid_X, b_test_X, b_valid_y, b_test_y = train_test_split(b_temp_X, b_temp_y, random_state=SEED, test_size=0.5, stratify=temp_y)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:31:21.011383Z","iopub.execute_input":"2024-05-01T14:31:21.011717Z","iopub.status.idle":"2024-05-01T14:31:21.039068Z","shell.execute_reply.started":"2024-05-01T14:31:21.011692Z","shell.execute_reply":"2024-05-01T14:31:21.038316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Return size of the split datasets\n(len(b_train_X), len(b_train_y)), (len(b_valid_X), len(b_valid_y)), (len(b_test_X), len(b_test_y))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:31:22.540919Z","iopub.execute_input":"2024-05-01T14:31:22.541265Z","iopub.status.idle":"2024-05-01T14:31:22.5476Z","shell.execute_reply.started":"2024-05-01T14:31:22.54124Z","shell.execute_reply":"2024-05-01T14:31:22.546776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import BERT Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModel, BertTokenizerFast","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:31:25.081197Z","iopub.execute_input":"2024-05-01T14:31:25.081547Z","iopub.status.idle":"2024-05-01T14:31:31.845892Z","shell.execute_reply.started":"2024-05-01T14:31:25.081521Z","shell.execute_reply":"2024-05-01T14:31:31.845073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:32:31.95035Z","iopub.execute_input":"2024-05-01T14:32:31.951082Z","iopub.status.idle":"2024-05-01T14:32:31.954892Z","shell.execute_reply.started":"2024-05-01T14:32:31.95105Z","shell.execute_reply":"2024-05-01T14:32:31.953957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# specify GPU\ndevice = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:32:35.079499Z","iopub.execute_input":"2024-05-01T14:32:35.080528Z","iopub.status.idle":"2024-05-01T14:32:35.084863Z","shell.execute_reply.started":"2024-05-01T14:32:35.080491Z","shell.execute_reply":"2024-05-01T14:32:35.083764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import BERT-base pretrained model\nbert = AutoModel.from_pretrained('bert-base-uncased')\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:32:38.940747Z","iopub.execute_input":"2024-05-01T14:32:38.94146Z","iopub.status.idle":"2024-05-01T14:32:43.460065Z","shell.execute_reply.started":"2024-05-01T14:32:38.941431Z","shell.execute_reply":"2024-05-01T14:32:43.459074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT Tokenization of the split datasets","metadata":{"execution":{"iopub.status.busy":"2024-04-16T10:06:10.90381Z","iopub.execute_input":"2024-04-16T10:06:10.904336Z","iopub.status.idle":"2024-04-16T10:06:10.910245Z","shell.execute_reply.started":"2024-04-16T10:06:10.904295Z","shell.execute_reply":"2024-04-16T10:06:10.908909Z"}}},{"cell_type":"code","source":"# Tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    b_train_X.tolist(),\n    max_length = MAX_LENGTH,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# Tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    b_valid_X.tolist(),\n    max_length = MAX_LENGTH,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# Tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    b_test_X.tolist(),\n    max_length = MAX_LENGTH,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:32:50.752986Z","iopub.execute_input":"2024-05-01T14:32:50.753585Z","iopub.status.idle":"2024-05-01T14:33:17.665389Z","shell.execute_reply.started":"2024-05-01T14:32:50.753556Z","shell.execute_reply":"2024-05-01T14:33:17.664586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert Integer Sequences to Tensors","metadata":{}},{"cell_type":"code","source":"# For train set\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\nb_train_y_ = torch.tensor(b_train_y.tolist())\n\n# For validation set\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nb_val_y_ = torch.tensor(b_valid_y.tolist())\n\n# For test set\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\nb_test_y_ = torch.tensor(b_test_y.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:33:24.609701Z","iopub.execute_input":"2024-05-01T14:33:24.61044Z","iopub.status.idle":"2024-05-01T14:33:27.112272Z","shell.execute_reply.started":"2024-05-01T14:33:24.610406Z","shell.execute_reply":"2024-05-01T14:33:27.1115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create DataLoaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:33:28.580823Z","iopub.execute_input":"2024-05-01T14:33:28.581437Z","iopub.status.idle":"2024-05-01T14:33:28.587807Z","shell.execute_reply.started":"2024-05-01T14:33:28.581397Z","shell.execute_reply":"2024-05-01T14:33:28.586282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a batch size\nbatch_size = 32\ntorch.manual_seed(SEED)\n\n# train_data\ntrain_data = TensorDataset(train_seq, train_mask, b_train_y_)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# val_data\nval_data = TensorDataset(val_seq, val_mask, b_val_y_)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n\n# test_data\ntest_data = TensorDataset(test_seq, test_mask, b_test_y_)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:34:10.337912Z","iopub.execute_input":"2024-05-01T14:34:10.338302Z","iopub.status.idle":"2024-05-01T14:34:10.347912Z","shell.execute_reply.started":"2024-05-01T14:34:10.338275Z","shell.execute_reply":"2024-05-01T14:34:10.347155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Freeze BERT Parameters","metadata":{}},{"cell_type":"code","source":"# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:34:13.726706Z","iopub.execute_input":"2024-05-01T14:34:13.72734Z","iopub.status.idle":"2024-05-01T14:34:13.732922Z","shell.execute_reply.started":"2024-05-01T14:34:13.727312Z","shell.execute_reply":"2024-05-01T14:34:13.731993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define BERT Architecture","metadata":{}},{"cell_type":"code","source":"from torch import nn","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:35:03.313366Z","iopub.execute_input":"2024-05-01T14:35:03.314138Z","iopub.status.idle":"2024-05-01T14:35:03.319404Z","shell.execute_reply.started":"2024-05-01T14:35:03.314104Z","shell.execute_reply":"2024-05-01T14:35:03.318217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n\n        super(BERT_Arch, self).__init__()\n        \n        self.bert = bert \n        self.dropout = nn.Dropout(0.1)\n        self.relu =  nn.ReLU()\n        self.fc1 = nn.Linear(768,512)\n        self.fc2 = nn.Linear(512,2)\n\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n\n        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n        x = self.fc1(cls_hs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        x = self.softmax(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:35:05.559117Z","iopub.execute_input":"2024-05-01T14:35:05.559477Z","iopub.status.idle":"2024-05-01T14:35:05.56752Z","shell.execute_reply.started":"2024-05-01T14:35:05.55945Z","shell.execute_reply":"2024-05-01T14:35:05.56651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:35:08.429459Z","iopub.execute_input":"2024-05-01T14:35:08.430169Z","iopub.status.idle":"2024-05-01T14:35:08.821824Z","shell.execute_reply.started":"2024-05-01T14:35:08.430135Z","shell.execute_reply":"2024-05-01T14:35:08.821066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer from hugging face transformers\nfrom transformers import AdamW\n\n# define the optimizer\noptimizer = AdamW(model.parameters(), lr = 1e-3)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:35:12.48737Z","iopub.execute_input":"2024-05-01T14:35:12.487725Z","iopub.status.idle":"2024-05-01T14:35:12.509189Z","shell.execute_reply.started":"2024-05-01T14:35:12.487696Z","shell.execute_reply":"2024-05-01T14:35:12.508248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Class Weights","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_y), y=train_y)\n\nprint(class_wts)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:35:15.211705Z","iopub.execute_input":"2024-05-01T14:35:15.21243Z","iopub.status.idle":"2024-05-01T14:35:15.225691Z","shell.execute_reply.started":"2024-05-01T14:35:15.2124Z","shell.execute_reply":"2024-05-01T14:35:15.224803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert class weights to tensor\nweights = torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\ncross_entropy  = nn.NLLLoss(weight=weights)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:35:18.800222Z","iopub.execute_input":"2024-05-01T14:35:18.801034Z","iopub.status.idle":"2024-05-01T14:35:18.810132Z","shell.execute_reply.started":"2024-05-01T14:35:18.800992Z","shell.execute_reply":"2024-05-01T14:35:18.809303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of training epochs\nBERT_EPOCHS = 40","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:39:42.18107Z","iopub.execute_input":"2024-05-01T14:39:42.181819Z","iopub.status.idle":"2024-05-01T14:39:42.185827Z","shell.execute_reply.started":"2024-05-01T14:39:42.181787Z","shell.execute_reply":"2024-05-01T14:39:42.184895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine tuning BERT","metadata":{}},{"cell_type":"code","source":"# function to train the model\ndef train():\n\n    model.train()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step, batch in enumerate(train_dataloader):\n\n        # progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print(\"  Batch {:>5,}  of  {:>5,}.\".format(step, len(train_dataloader)))\n\n        # push the batch to gpu\n        batch = [r.to(device) for r in batch]\n\n        sent_id, mask, labels = batch\n\n        # clear previously calculated gradients\n        model.zero_grad()\n\n        # get model predictions for the current batch\n        preds = model(sent_id, mask)\n\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n\n        # backward pass to calculate the gradients\n        loss.backward()\n\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # update parameters\n        optimizer.step()\n\n        # model predictions are stored on GPU. So, push it to CPU\n        preds = preds.detach().cpu().numpy()\n\n        # append the model predictions\n        total_preds.append(preds)\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss / len(train_dataloader)\n\n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds = np.concatenate(total_preds, axis=0)\n\n    # returns the loss and predictions\n    return avg_loss, total_preds","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:35:34.538568Z","iopub.execute_input":"2024-05-01T14:35:34.539407Z","iopub.status.idle":"2024-05-01T14:35:34.551436Z","shell.execute_reply.started":"2024-05-01T14:35:34.539367Z","shell.execute_reply":"2024-05-01T14:35:34.550498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n\n    print(\"\\nEvaluating...\")\n\n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step, batch in enumerate(val_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n\n            # Report progress.\n            print(\"  Batch {:>5,}  of  {:>5,}.\".format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            preds = model(sent_id, mask)\n\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds, labels)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n\n            total_preds.append(preds)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(val_dataloader)\n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:35:35.65484Z","iopub.execute_input":"2024-05-01T14:35:35.655414Z","iopub.status.idle":"2024-05-01T14:35:35.663891Z","shell.execute_reply.started":"2024-05-01T14:35:35.655382Z","shell.execute_reply":"2024-05-01T14:35:35.662914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT Model Training","metadata":{}},{"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float(\"inf\")\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses = []\nvalid_losses = []\n\n# early stop\nEARLY_STOP = 5\n\nes_counter = 0\n\n# for each epoch\nfor epoch in tqdm(range(BERT_EPOCHS)):\n\n    print(\"\\n Epoch {:} / {:}\".format(epoch + 1, BERT_EPOCHS))\n\n    # train model\n    train_loss, _ = train()\n\n    # evaluate model\n    valid_loss, _ = evaluate()\n\n    # save the best model\n    if valid_loss < best_valid_loss:\n        # Reset early stop\n        es_counter = 0\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), \"saved_weights.pt\")\n\n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n\n    print(f\"\\nTraining Loss: {train_loss:.3f}\")\n    print(f\"Validation Loss: {valid_loss:.3f}\")\n    \n    es_counter += 1\n    if es_counter == EARLY_STOP:\n        print(f\"Training stopped early at epoch {epoch + 1}\")\n        break","metadata":{"execution":{"iopub.status.busy":"2024-05-01T14:40:39.746396Z","iopub.execute_input":"2024-05-01T14:40:39.746967Z","iopub.status.idle":"2024-05-01T15:53:21.690236Z","shell.execute_reply.started":"2024-05-01T14:40:39.746938Z","shell.execute_reply":"2024-05-01T15:53:21.689342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Performance","metadata":{}},{"cell_type":"code","source":"# Visualise the train_loss vs valid_loss\ndata_preproc = pd.DataFrame({\n    'train_loss': train_losses,\n    'valid_loss': valid_losses})","metadata":{"execution":{"iopub.status.busy":"2024-05-01T16:05:47.016173Z","iopub.execute_input":"2024-05-01T16:05:47.016513Z","iopub.status.idle":"2024-05-01T16:05:47.021691Z","shell.execute_reply.started":"2024-05-01T16:05:47.016488Z","shell.execute_reply":"2024-05-01T16:05:47.020716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_plot = sns.lineplot(data_preproc)\nloss_plot.set(xlabel='Epoch', ylabel='Loss');","metadata":{"execution":{"iopub.status.busy":"2024-05-01T16:11:15.891204Z","iopub.execute_input":"2024-05-01T16:11:15.892142Z","iopub.status.idle":"2024-05-01T16:11:16.271391Z","shell.execute_reply.started":"2024-05-01T16:11:15.892106Z","shell.execute_reply":"2024-05-01T16:11:16.270358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# load weights of best model\npath = 'saved_weights.pt'\nmodel.load_state_dict(torch.load(path))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T16:20:04.061365Z","iopub.execute_input":"2024-05-01T16:20:04.061742Z","iopub.status.idle":"2024-05-01T16:20:04.419916Z","shell.execute_reply.started":"2024-05-01T16:20:04.061712Z","shell.execute_reply":"2024-05-01T16:20:04.418979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2024-05-01T16:16:50.314855Z","iopub.execute_input":"2024-05-01T16:16:50.315575Z","iopub.status.idle":"2024-05-01T16:16:50.321292Z","shell.execute_reply.started":"2024-05-01T16:16:50.315542Z","shell.execute_reply":"2024-05-01T16:16:50.320333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predictions for test data\nwith torch.no_grad():\n#     preds = model(test_seq.to(device), test_mask.to(device))\n#     preds = preds.detach().cpu().numpy()    \n\n    cross_matrix = []\n    \n    for step, batch in tqdm(enumerate(test_dataloader)):\n\n#         if step > 0:\n#             break\n\n        # push the batch to gpu\n        batch = [r.to(device) for r in batch]\n\n        sent_id, mask, labels = batch\n\n        # get model predictions for the current batch\n        preds = model(sent_id, mask)\n        preds = preds.detach().cpu().numpy()\n        \n        # model's performance\n        preds = np.argmax(preds, axis = 1)\n        print(classification_report(labels.cpu(), preds))\n        \n        # confusion matrix\n        cross_matrix.append(pd.crosstab(labels.cpu(), preds))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T16:22:18.454737Z","iopub.execute_input":"2024-05-01T16:22:18.45512Z","iopub.status.idle":"2024-05-01T16:22:39.127738Z","shell.execute_reply.started":"2024-05-01T16:22:18.45509Z","shell.execute_reply":"2024-05-01T16:22:39.126742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_cm = sum(cross_matrix)\nfinal_cm","metadata":{"execution":{"iopub.status.busy":"2024-05-01T16:23:25.507869Z","iopub.execute_input":"2024-05-01T16:23:25.508572Z","iopub.status.idle":"2024-05-01T16:23:25.551321Z","shell.execute_reply.started":"2024-05-01T16:23:25.508537Z","shell.execute_reply":"2024-05-01T16:23:25.550341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (6,6))\nsns.heatmap(final_cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['True','Fake'] , yticklabels = ['True','Fake'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\");","metadata":{"execution":{"iopub.status.busy":"2024-05-01T16:23:31.943098Z","iopub.execute_input":"2024-05-01T16:23:31.943713Z","iopub.status.idle":"2024-05-01T16:23:32.25297Z","shell.execute_reply.started":"2024-05-01T16:23:31.943683Z","shell.execute_reply":"2024-05-01T16:23:32.252055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (6,6))\nsns.heatmap(final_cm/3864*100,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['True','Fake'] , yticklabels = ['True','Fake'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\");","metadata":{"execution":{"iopub.status.busy":"2024-05-01T16:23:44.056056Z","iopub.execute_input":"2024-05-01T16:23:44.056412Z","iopub.status.idle":"2024-05-01T16:23:44.350883Z","shell.execute_reply.started":"2024-05-01T16:23:44.056383Z","shell.execute_reply":"2024-05-01T16:23:44.349967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Thoughts","metadata":{}},{"cell_type":"markdown","source":"The BERT model performs slightly worse than the LSTM model. The complexity of the model might be the reason for not being able to converge to the optimum results. Additionally, tuning the learning rate and other hyperparameters could further improve the performance of the model. The LSTM model also benefitted from learning on pre-trained embeddings while removing stopwords and unintelligible words.","metadata":{}}]}